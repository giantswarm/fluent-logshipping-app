clusterID: clusterID

global:
  podSecurityStandards:
    enforced: false

registry:
  domain: gsoci.azurecr.io

fluentbit:
  priorityClass:
    enabled: false
    create: false
    # The value is only relevant when using the custom priority class.
    value: 200000000
    name: fluentbit

  port: 5170
  protocol: TCP

  # For systemd sshd logs it needs root
  userID: 0
  # For systemd sshd logs it needs root
  groupID: 0

  image:
    name: giantswarm/fluent-bit-audited
    # -- Overrides the image tag whose default is the chart's appVersion
    # We use a custom version here that is built here https://github.com/giantswarm/fluent-bit.
    # This version adds a shell in the container so we can use the exec plugin as well as install ausearch.
    # It is based on version 3.1.6 of fluent-bit.
    tag: "0.4.1"

  logLevel: info
  flushFrequencyInSeconds: 5
  backlogMemLimit: "50M"
  memBufferLimit: "10MB"

  # -- Persistence settings to store the fluent bit databases and filesystem buffering if enabled. Empty Dir is used by default.
  persistence:
    pvc:
      enabled: false
      # -- Name of the PVC to use if it exists in the cluster already.
      # existingClaim: ""
      accessModes:
      - ReadWriteOnce
      size: 10Gi
      # storageClassName: default
    emptyDir:
      # -- Value is either disk ("") or memory "Memory". Default is disk
      medium: ""
      ## 1 Gi is enough for the databases but this can be increased if buffering on the filesystem is used.
      sizeLimit: 1Gi

  storageMaxChunksUp: 128

  buffering:
    audit: "memory"
    containers: "memory"
    sshd: "memory"
    syslog: "memory"

serviceAccount:
  annotations: {}

outputs:
  inputLogTypes: &logTypes
    - execve
    - sshd
    - containers
    - syslog
    - audit
  aws:
    kiam: false
    region: ""
    role: ""
    credentials:
      awsAccessKey: ""
      awsSecretKey: ""
    cloudWatch:
      enabled: false
      inputLogTypes: *logTypes
      logGroupName: "my-cluster"
      logStreamName: "example-stream"
      logRetentionDays: -1
    S3:
      enabled: false
      inputLogTypes: *logTypes
      bucketName: "my-cluster-logs"
      bucketPathPrefix: ""
      endpoint: ""
      totalFileSize: "100M"
      s3_object_key_format: "/$TAG/%Y/%m/%d/%H/%M/%S"

  elasticsearch:
    enabled: false
    inputLogTypes: *logTypes
    host: ""
    port: 9200
    path: "/"
    tlsEnabled: true
    sslVerify: true
    secured: false
    user: ""
    password: ""
    config:
      audit:
        ssh:
          indexName: audit-ssh
          # suppressTypeName: false
        kubernetes:
          indexName: audit-kubernetes
          # suppressTypeName: false
      containers:
        indexName: kubernetes
        # suppressTypeName: false
      syslog:
        indexName: syslog
        # suppressTypeName: false

  extraOutputConfig: ""

extraFilters:
  preprocessing: ""
  postprocessing: ""

serviceMonitor:
  enabled: true
  # -- (duration) Prometheus scrape interval.
  interval: "60s"
  # -- (duration) Prometheus scrape timeout.
  scrapeTimeout: "45s"

# Enable Kyverno PolicyException
kyvernoPolicyExceptions:
  enabled: true
  namespace: giantswarm

# Tolerate all nodes with NoSchedule taints
tolerations:
  - operator: "Exists"
    effect: "NoSchedule"

resources:
  limits:
    memory: 200Mi
  requests:
    cpu: 500m
    memory: 100Mi

verticalPodAutoscaler:
  enabled: true
