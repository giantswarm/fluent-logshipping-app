# Default values for fluent-logshipping-app.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# clusterID is dynamic environment value, calculated after cluster creation
# applies only to Giant Swarm clusters
clusterID: clusterID

global:
  podSecurityStandards:
    enforced: false

project:
  branch: "[[ .Branch ]]"
  commit: "[[ .SHA ]]"

registry:
  domain: docker.io

fluentbit:
  port: 5170
  protocol: TCP

  # For systemd sshd logs it needs root
  userID: 0
  # For systemd sshd logs it needs root
  groupID: 0

  image:
    name: giantswarm/fluent-bit-audited
    # -- Overrides the image tag whose default is the chart's appVersion
    # We use a custom version here that is built here https://github.com/giantswarm/fluent-bit/tree/v0.1.0.
    # This version adds a shell in the container so we can use the exec plugin as well as install ausearch.
    # It is based on version 2.2.0 of fluent-bit.
    tag: "0.2.0"

  logLevel: info
  flushFrequencyInSeconds: 5
  backlogMemLimit: "50M"
  memBufferLimit: "10MB"

  storage:
    sizeLimit: 15Gi

  storageMaxChunksUp: 128

  inputStorageTypes:
    audit: "memory"
    container: "memory"
    sshd: "memory"
    syslog: "memory"

serviceAccount:
  annotations: {}

outputs:
  inputLogTypes: &logTypes
  - sshd
  - containers
  - syslog
  - audit
  aws:
    kiam: false
    role: ""
    region: ""
    credentials:
      awsAccessKey: ""
      awsSecretKey: ""
    cloudWatch:
      enabled: false
      inputLogTypes: *logTypes
      logGroupName: "my-cluster"
      logStreamName: "example-stream"
      logRetentionDays: -1
    S3:
      enabled: false
      inputLogTypes: *logTypes
      bucketName: "my-cluster-logs"
      bucketPathPrefix: ""
      endpoint: ""
      totalFileSize: "100M"
      s3_object_key_format: "/$TAG/%Y/%m/%d/%H/%M/%S"

  azure:
    logAnalytics:
      enabled: false
      inputLogTypes: *logTypes
      workspaceId: ""
      sharedKey: ""
      customLogs:
        audit:
          ssh:
            name: SshAuditLogs
          kubernetes:
            name: KubernetesAuditLogs
        containers:
          name: KubernetesContainerLogs
        syslog:
          name: KubernetesSyslog

  elasticsearch:
    enabled: false
    inputLogTypes: *logTypes
    host: ""
    port: 9200
    path: "/"
    tlsEnabled: true
    sslVerify: true
    secured: false
    user: ""
    password: ""
    indices:
      audit:
        ssh:
          name: audit-ssh
        kubernetes:
          name: audit-kubernetes
      containers:
        name: kubernetes
      syslog:
        name: syslog

  tcp:
    enabled: false
    host: ""
    port: ""
    # Specify the data format to be printed. Supported formats are msgpack, json, json_lines and json_stream.
    format: json
    # Specify the name of the time key in the output record. To disable the time key just set the value to false.
    json_date_key: date
    # Specify the format of the date. Supported formats are double, epoch and iso8601 (eg: 2018-05-30T09:39:52.000681Z)
    json_date_format: double
    inputLogTypes: *logTypes
    # This output currently only supports the forwarding of syslog messages.
    indices:
      syslog:
        name: syslog

  extraOutputConfig: ""

extraFilters: ""

serviceMonitor:
  enabled: true
  # -- (duration) Prometheus scrape interval.
  interval: "60s"
  # -- (duration) Prometheus scrape timeout.
  scrapeTimeout: "45s"

# Enable Kyverno PolicyException
kyvernoPolicyExceptions:
  enabled: true
  namespace: giantswarm

# Tolerate all nodes with NoSchedule taints
tolerations:
  - operator: "Exists"
    effect: "NoSchedule"

resources:
  limits:
    memory: 200Mi
  requests:
    cpu: 500m
    memory: 100Mi

verticalPodAutoscaler:
  enabled: true
